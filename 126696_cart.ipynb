{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NckolasGomes6696/My-stuff/blob/main/126696_cart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty6rpKVpvdWV",
        "outputId": "f30a62ce-f04b-4303-9ba6-63d0963d061c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please import `sobel` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episódio 1/1000 - Recompensa: 13.0\n",
            "Episódio 2/1000 - Recompensa: 21.0\n",
            "Episódio 3/1000 - Recompensa: 11.0\n",
            "Episódio 4/1000 - Recompensa: 10.0\n",
            "Episódio 5/1000 - Recompensa: 34.0\n",
            "Episódio 6/1000 - Recompensa: 11.0\n",
            "Episódio 7/1000 - Recompensa: 23.0\n",
            "Episódio 8/1000 - Recompensa: 8.0\n",
            "Episódio 9/1000 - Recompensa: 10.0\n",
            "Episódio 10/1000 - Recompensa: 8.0\n",
            "Episódio 11/1000 - Recompensa: 9.0\n",
            "Episódio 12/1000 - Recompensa: 10.0\n",
            "Episódio 13/1000 - Recompensa: 9.0\n",
            "Episódio 14/1000 - Recompensa: 10.0\n",
            "Episódio 15/1000 - Recompensa: 9.0\n",
            "Episódio 16/1000 - Recompensa: 10.0\n",
            "Episódio 17/1000 - Recompensa: 10.0\n",
            "Episódio 18/1000 - Recompensa: 8.0\n",
            "Episódio 19/1000 - Recompensa: 9.0\n",
            "Episódio 20/1000 - Recompensa: 9.0\n",
            "Episódio 21/1000 - Recompensa: 11.0\n",
            "Episódio 22/1000 - Recompensa: 9.0\n",
            "Episódio 23/1000 - Recompensa: 9.0\n",
            "Episódio 24/1000 - Recompensa: 8.0\n",
            "Episódio 25/1000 - Recompensa: 10.0\n",
            "Episódio 26/1000 - Recompensa: 10.0\n",
            "Episódio 27/1000 - Recompensa: 9.0\n",
            "Episódio 28/1000 - Recompensa: 8.0\n",
            "Episódio 29/1000 - Recompensa: 10.0\n",
            "Episódio 30/1000 - Recompensa: 9.0\n",
            "Episódio 31/1000 - Recompensa: 10.0\n",
            "Episódio 32/1000 - Recompensa: 9.0\n",
            "Episódio 33/1000 - Recompensa: 10.0\n",
            "Episódio 34/1000 - Recompensa: 9.0\n",
            "Episódio 35/1000 - Recompensa: 9.0\n",
            "Episódio 36/1000 - Recompensa: 10.0\n",
            "Episódio 37/1000 - Recompensa: 8.0\n",
            "Episódio 38/1000 - Recompensa: 10.0\n",
            "Episódio 39/1000 - Recompensa: 9.0\n",
            "Episódio 40/1000 - Recompensa: 10.0\n",
            "Episódio 41/1000 - Recompensa: 8.0\n",
            "Episódio 42/1000 - Recompensa: 9.0\n",
            "Episódio 43/1000 - Recompensa: 9.0\n",
            "Episódio 44/1000 - Recompensa: 10.0\n",
            "Episódio 45/1000 - Recompensa: 10.0\n",
            "Episódio 46/1000 - Recompensa: 10.0\n",
            "Episódio 47/1000 - Recompensa: 9.0\n",
            "Episódio 48/1000 - Recompensa: 10.0\n",
            "Episódio 49/1000 - Recompensa: 10.0\n",
            "Episódio 50/1000 - Recompensa: 9.0\n",
            "Episódio 51/1000 - Recompensa: 9.0\n",
            "Episódio 52/1000 - Recompensa: 10.0\n",
            "Episódio 53/1000 - Recompensa: 9.0\n",
            "Episódio 54/1000 - Recompensa: 10.0\n",
            "Episódio 55/1000 - Recompensa: 9.0\n",
            "Episódio 56/1000 - Recompensa: 10.0\n",
            "Episódio 57/1000 - Recompensa: 10.0\n",
            "Episódio 58/1000 - Recompensa: 9.0\n",
            "Episódio 59/1000 - Recompensa: 10.0\n",
            "Episódio 60/1000 - Recompensa: 9.0\n",
            "Episódio 61/1000 - Recompensa: 9.0\n",
            "Episódio 62/1000 - Recompensa: 9.0\n",
            "Episódio 63/1000 - Recompensa: 10.0\n",
            "Episódio 64/1000 - Recompensa: 9.0\n",
            "Episódio 65/1000 - Recompensa: 9.0\n",
            "Episódio 66/1000 - Recompensa: 8.0\n",
            "Episódio 67/1000 - Recompensa: 9.0\n",
            "Episódio 68/1000 - Recompensa: 10.0\n",
            "Episódio 69/1000 - Recompensa: 9.0\n",
            "Episódio 70/1000 - Recompensa: 9.0\n",
            "Episódio 71/1000 - Recompensa: 10.0\n",
            "Episódio 72/1000 - Recompensa: 9.0\n",
            "Episódio 73/1000 - Recompensa: 10.0\n",
            "Episódio 74/1000 - Recompensa: 8.0\n",
            "Episódio 75/1000 - Recompensa: 10.0\n",
            "Episódio 76/1000 - Recompensa: 9.0\n",
            "Episódio 77/1000 - Recompensa: 10.0\n",
            "Episódio 78/1000 - Recompensa: 9.0\n",
            "Episódio 79/1000 - Recompensa: 8.0\n",
            "Episódio 80/1000 - Recompensa: 10.0\n",
            "Episódio 81/1000 - Recompensa: 8.0\n",
            "Episódio 82/1000 - Recompensa: 9.0\n",
            "Episódio 83/1000 - Recompensa: 9.0\n",
            "Episódio 84/1000 - Recompensa: 9.0\n",
            "Episódio 85/1000 - Recompensa: 10.0\n",
            "Episódio 86/1000 - Recompensa: 9.0\n",
            "Episódio 87/1000 - Recompensa: 8.0\n",
            "Episódio 88/1000 - Recompensa: 10.0\n",
            "Episódio 89/1000 - Recompensa: 10.0\n",
            "Episódio 90/1000 - Recompensa: 8.0\n",
            "Episódio 91/1000 - Recompensa: 8.0\n",
            "Episódio 92/1000 - Recompensa: 10.0\n",
            "Episódio 93/1000 - Recompensa: 9.0\n",
            "Episódio 94/1000 - Recompensa: 10.0\n",
            "Episódio 95/1000 - Recompensa: 9.0\n",
            "Episódio 96/1000 - Recompensa: 9.0\n",
            "Episódio 97/1000 - Recompensa: 9.0\n",
            "Episódio 98/1000 - Recompensa: 10.0\n",
            "Episódio 99/1000 - Recompensa: 9.0\n",
            "Episódio 100/1000 - Recompensa: 9.0\n",
            "Episódio 101/1000 - Recompensa: 10.0\n",
            "Episódio 102/1000 - Recompensa: 10.0\n",
            "Episódio 103/1000 - Recompensa: 9.0\n",
            "Episódio 104/1000 - Recompensa: 8.0\n",
            "Episódio 105/1000 - Recompensa: 9.0\n",
            "Episódio 106/1000 - Recompensa: 9.0\n",
            "Episódio 107/1000 - Recompensa: 10.0\n",
            "Episódio 108/1000 - Recompensa: 9.0\n",
            "Episódio 109/1000 - Recompensa: 9.0\n",
            "Episódio 110/1000 - Recompensa: 9.0\n",
            "Episódio 111/1000 - Recompensa: 9.0\n",
            "Episódio 112/1000 - Recompensa: 9.0\n",
            "Episódio 113/1000 - Recompensa: 8.0\n",
            "Episódio 114/1000 - Recompensa: 10.0\n",
            "Episódio 115/1000 - Recompensa: 9.0\n",
            "Episódio 116/1000 - Recompensa: 8.0\n",
            "Episódio 117/1000 - Recompensa: 10.0\n",
            "Episódio 118/1000 - Recompensa: 10.0\n",
            "Episódio 119/1000 - Recompensa: 10.0\n",
            "Episódio 120/1000 - Recompensa: 8.0\n",
            "Episódio 121/1000 - Recompensa: 10.0\n",
            "Episódio 122/1000 - Recompensa: 8.0\n",
            "Episódio 123/1000 - Recompensa: 8.0\n",
            "Episódio 124/1000 - Recompensa: 10.0\n",
            "Episódio 125/1000 - Recompensa: 9.0\n",
            "Episódio 126/1000 - Recompensa: 9.0\n",
            "Episódio 127/1000 - Recompensa: 9.0\n",
            "Episódio 128/1000 - Recompensa: 8.0\n",
            "Episódio 129/1000 - Recompensa: 10.0\n",
            "Episódio 130/1000 - Recompensa: 8.0\n",
            "Episódio 131/1000 - Recompensa: 10.0\n",
            "Episódio 132/1000 - Recompensa: 9.0\n",
            "Episódio 133/1000 - Recompensa: 10.0\n",
            "Episódio 134/1000 - Recompensa: 9.0\n",
            "Episódio 135/1000 - Recompensa: 10.0\n",
            "Episódio 136/1000 - Recompensa: 9.0\n",
            "Episódio 137/1000 - Recompensa: 9.0\n",
            "Episódio 138/1000 - Recompensa: 10.0\n",
            "Episódio 139/1000 - Recompensa: 8.0\n",
            "Episódio 140/1000 - Recompensa: 8.0\n",
            "Episódio 141/1000 - Recompensa: 10.0\n",
            "Episódio 142/1000 - Recompensa: 9.0\n",
            "Episódio 143/1000 - Recompensa: 8.0\n",
            "Episódio 144/1000 - Recompensa: 8.0\n",
            "Episódio 145/1000 - Recompensa: 9.0\n",
            "Episódio 146/1000 - Recompensa: 11.0\n",
            "Episódio 147/1000 - Recompensa: 10.0\n",
            "Episódio 148/1000 - Recompensa: 10.0\n",
            "Episódio 149/1000 - Recompensa: 10.0\n",
            "Episódio 150/1000 - Recompensa: 10.0\n",
            "Episódio 151/1000 - Recompensa: 9.0\n",
            "Episódio 152/1000 - Recompensa: 10.0\n",
            "Episódio 153/1000 - Recompensa: 10.0\n",
            "Episódio 154/1000 - Recompensa: 9.0\n",
            "Episódio 155/1000 - Recompensa: 9.0\n",
            "Episódio 156/1000 - Recompensa: 10.0\n",
            "Episódio 157/1000 - Recompensa: 9.0\n",
            "Episódio 158/1000 - Recompensa: 8.0\n",
            "Episódio 159/1000 - Recompensa: 10.0\n",
            "Episódio 160/1000 - Recompensa: 9.0\n",
            "Episódio 161/1000 - Recompensa: 9.0\n",
            "Episódio 162/1000 - Recompensa: 9.0\n",
            "Episódio 163/1000 - Recompensa: 9.0\n",
            "Episódio 164/1000 - Recompensa: 10.0\n",
            "Episódio 165/1000 - Recompensa: 10.0\n",
            "Episódio 166/1000 - Recompensa: 9.0\n",
            "Episódio 167/1000 - Recompensa: 10.0\n",
            "Episódio 168/1000 - Recompensa: 10.0\n",
            "Episódio 169/1000 - Recompensa: 9.0\n",
            "Episódio 170/1000 - Recompensa: 9.0\n",
            "Episódio 171/1000 - Recompensa: 9.0\n",
            "Episódio 172/1000 - Recompensa: 10.0\n",
            "Episódio 173/1000 - Recompensa: 10.0\n",
            "Episódio 174/1000 - Recompensa: 9.0\n",
            "Episódio 175/1000 - Recompensa: 8.0\n",
            "Episódio 176/1000 - Recompensa: 8.0\n",
            "Episódio 177/1000 - Recompensa: 10.0\n",
            "Episódio 178/1000 - Recompensa: 9.0\n",
            "Episódio 179/1000 - Recompensa: 9.0\n",
            "Episódio 180/1000 - Recompensa: 8.0\n",
            "Episódio 181/1000 - Recompensa: 10.0\n",
            "Episódio 182/1000 - Recompensa: 10.0\n",
            "Episódio 183/1000 - Recompensa: 9.0\n",
            "Episódio 184/1000 - Recompensa: 10.0\n",
            "Episódio 185/1000 - Recompensa: 9.0\n",
            "Episódio 186/1000 - Recompensa: 9.0\n",
            "Episódio 187/1000 - Recompensa: 9.0\n",
            "Episódio 188/1000 - Recompensa: 9.0\n",
            "Episódio 189/1000 - Recompensa: 9.0\n",
            "Episódio 190/1000 - Recompensa: 10.0\n",
            "Episódio 191/1000 - Recompensa: 9.0\n",
            "Episódio 192/1000 - Recompensa: 9.0\n",
            "Episódio 193/1000 - Recompensa: 9.0\n",
            "Episódio 194/1000 - Recompensa: 10.0\n",
            "Episódio 195/1000 - Recompensa: 10.0\n",
            "Episódio 196/1000 - Recompensa: 10.0\n",
            "Episódio 197/1000 - Recompensa: 10.0\n",
            "Episódio 198/1000 - Recompensa: 11.0\n",
            "Episódio 199/1000 - Recompensa: 11.0\n",
            "Episódio 200/1000 - Recompensa: 9.0\n",
            "Episódio 201/1000 - Recompensa: 9.0\n",
            "Episódio 202/1000 - Recompensa: 9.0\n",
            "Episódio 203/1000 - Recompensa: 9.0\n",
            "Episódio 204/1000 - Recompensa: 10.0\n",
            "Episódio 205/1000 - Recompensa: 10.0\n",
            "Episódio 206/1000 - Recompensa: 8.0\n",
            "Episódio 207/1000 - Recompensa: 10.0\n",
            "Episódio 208/1000 - Recompensa: 9.0\n",
            "Episódio 209/1000 - Recompensa: 9.0\n",
            "Episódio 210/1000 - Recompensa: 10.0\n",
            "Episódio 211/1000 - Recompensa: 9.0\n",
            "Episódio 212/1000 - Recompensa: 9.0\n",
            "Episódio 213/1000 - Recompensa: 8.0\n",
            "Episódio 214/1000 - Recompensa: 8.0\n",
            "Episódio 215/1000 - Recompensa: 10.0\n",
            "Episódio 216/1000 - Recompensa: 9.0\n",
            "Episódio 217/1000 - Recompensa: 9.0\n",
            "Episódio 218/1000 - Recompensa: 10.0\n",
            "Episódio 219/1000 - Recompensa: 9.0\n",
            "Episódio 220/1000 - Recompensa: 10.0\n",
            "Episódio 221/1000 - Recompensa: 11.0\n",
            "Episódio 222/1000 - Recompensa: 9.0\n",
            "Episódio 223/1000 - Recompensa: 11.0\n",
            "Episódio 224/1000 - Recompensa: 10.0\n",
            "Episódio 225/1000 - Recompensa: 9.0\n",
            "Episódio 226/1000 - Recompensa: 9.0\n",
            "Episódio 227/1000 - Recompensa: 8.0\n",
            "Episódio 228/1000 - Recompensa: 9.0\n",
            "Episódio 229/1000 - Recompensa: 9.0\n",
            "Episódio 230/1000 - Recompensa: 10.0\n",
            "Episódio 231/1000 - Recompensa: 10.0\n",
            "Episódio 232/1000 - Recompensa: 9.0\n",
            "Episódio 233/1000 - Recompensa: 10.0\n",
            "Episódio 234/1000 - Recompensa: 9.0\n",
            "Episódio 235/1000 - Recompensa: 11.0\n",
            "Episódio 236/1000 - Recompensa: 9.0\n",
            "Episódio 237/1000 - Recompensa: 10.0\n",
            "Episódio 238/1000 - Recompensa: 8.0\n",
            "Episódio 239/1000 - Recompensa: 8.0\n",
            "Episódio 240/1000 - Recompensa: 10.0\n",
            "Episódio 241/1000 - Recompensa: 10.0\n",
            "Episódio 242/1000 - Recompensa: 8.0\n",
            "Episódio 243/1000 - Recompensa: 10.0\n",
            "Episódio 244/1000 - Recompensa: 10.0\n",
            "Episódio 245/1000 - Recompensa: 9.0\n",
            "Episódio 246/1000 - Recompensa: 9.0\n",
            "Episódio 247/1000 - Recompensa: 9.0\n",
            "Episódio 248/1000 - Recompensa: 9.0\n",
            "Episódio 249/1000 - Recompensa: 9.0\n",
            "Episódio 250/1000 - Recompensa: 9.0\n",
            "Episódio 251/1000 - Recompensa: 9.0\n",
            "Episódio 252/1000 - Recompensa: 9.0\n",
            "Episódio 253/1000 - Recompensa: 10.0\n",
            "Episódio 254/1000 - Recompensa: 8.0\n",
            "Episódio 255/1000 - Recompensa: 9.0\n",
            "Episódio 256/1000 - Recompensa: 9.0\n",
            "Episódio 257/1000 - Recompensa: 9.0\n",
            "Episódio 258/1000 - Recompensa: 11.0\n",
            "Episódio 259/1000 - Recompensa: 10.0\n",
            "Episódio 260/1000 - Recompensa: 9.0\n",
            "Episódio 261/1000 - Recompensa: 10.0\n",
            "Episódio 262/1000 - Recompensa: 9.0\n",
            "Episódio 263/1000 - Recompensa: 10.0\n",
            "Episódio 264/1000 - Recompensa: 8.0\n",
            "Episódio 265/1000 - Recompensa: 11.0\n",
            "Episódio 266/1000 - Recompensa: 9.0\n",
            "Episódio 267/1000 - Recompensa: 10.0\n",
            "Episódio 268/1000 - Recompensa: 9.0\n",
            "Episódio 269/1000 - Recompensa: 10.0\n",
            "Episódio 270/1000 - Recompensa: 10.0\n",
            "Episódio 271/1000 - Recompensa: 9.0\n",
            "Episódio 272/1000 - Recompensa: 10.0\n",
            "Episódio 273/1000 - Recompensa: 10.0\n",
            "Episódio 274/1000 - Recompensa: 9.0\n",
            "Episódio 275/1000 - Recompensa: 11.0\n",
            "Episódio 276/1000 - Recompensa: 10.0\n",
            "Episódio 277/1000 - Recompensa: 9.0\n",
            "Episódio 278/1000 - Recompensa: 9.0\n",
            "Episódio 279/1000 - Recompensa: 9.0\n",
            "Episódio 280/1000 - Recompensa: 9.0\n",
            "Episódio 281/1000 - Recompensa: 10.0\n",
            "Episódio 282/1000 - Recompensa: 10.0\n",
            "Episódio 283/1000 - Recompensa: 8.0\n",
            "Episódio 284/1000 - Recompensa: 10.0\n",
            "Episódio 285/1000 - Recompensa: 9.0\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import moviepy.editor as mpy\n",
        "\n",
        "# Definição do ambiente (CartPole)\n",
        "env = gym.make('CartPole-v1')\n",
        "num_actions = env.action_space.n  # Ações possíveis (esquerda/direita)\n",
        "\n",
        "# Modelo de Política (Actor)\n",
        "def create_actor_model():\n",
        "    inputs = layers.Input(shape=(env.observation_space.shape[0],))\n",
        "    hidden_layer = layers.Dense(128, activation='relu')(inputs)\n",
        "    hidden_layer = layers.Dense(128, activation='relu')(hidden_layer)\n",
        "    output = layers.Dense(num_actions, activation='softmax')(hidden_layer)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Modelo Crítico (Critic)\n",
        "def create_critic_model():\n",
        "    inputs = layers.Input(shape=(env.observation_space.shape[0],))\n",
        "    hidden_layer = layers.Dense(128, activation='relu')(inputs)\n",
        "    hidden_layer = layers.Dense(128, activation='relu')(hidden_layer)\n",
        "    output = layers.Dense(1)(hidden_layer)\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
        "    return model\n",
        "\n",
        "# Inicializando os modelos\n",
        "actor_model = create_actor_model()\n",
        "critic_model = create_critic_model()\n",
        "\n",
        "# Definir otimizadores\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "huber_loss = tf.keras.losses.Huber()\n",
        "\n",
        "# Função para calcular o reforço (reward) futuro esperado e atualizar os modelos\n",
        "def update_actor_critic(states, actions, advantages, returns):\n",
        "    with tf.GradientTape() as tape:\n",
        "        action_probs = actor_model(states, training=True)\n",
        "        critic_value = critic_model(states, training=True)\n",
        "\n",
        "        # Cálculo da perda de política (actor)\n",
        "        action_one_hot = tf.one_hot(actions, num_actions)\n",
        "        selected_action_probs = tf.reduce_sum(action_probs * action_one_hot, axis=1)\n",
        "        log_action_probs = tf.math.log(selected_action_probs + 1e-10)\n",
        "        actor_loss = -tf.reduce_sum(log_action_probs * advantages)\n",
        "\n",
        "        # Cálculo da perda do crítico\n",
        "        #critic_value = tf.squeeze(critic_value)  # Remover dimensões desnecessárias - This line is causing the error, since it converts critic_value to a scalar\n",
        "        #returns = tf.squeeze(returns)  # Remover dimensões desnecessárias - This line is causing the error, since it converts returns to a scalar\n",
        "        critic_loss = huber_loss(returns, critic_value)\n",
        "\n",
        "        total_loss = actor_loss + critic_loss\n",
        "\n",
        "    # Computar e aplicar gradientes\n",
        "    grads = tape.gradient(total_loss, actor_model.trainable_variables + critic_model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, actor_model.trainable_variables + critic_model.trainable_variables))\n",
        "\n",
        "# Função de treino\n",
        "def train(env, max_episodes=1000, gamma=0.99):\n",
        "    reward_history = []\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
        "\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Prever ação do ator\n",
        "            action_probs = actor_model(state, training=False)\n",
        "            action_probs = np.squeeze(action_probs)  # Garantir a forma correta\n",
        "            action = np.random.choice(num_actions, p=action_probs)\n",
        "\n",
        "            # Realizar a ação\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
        "\n",
        "            # Calcular valor do crítico\n",
        "            critic_value = critic_model(state)\n",
        "            next_critic_value = critic_model(next_state)\n",
        "\n",
        "            # Atualizar recompensa acumulada\n",
        "            episode_reward += reward\n",
        "\n",
        "            # Calcular o valor-alvo e vantagem\n",
        "            if done:\n",
        "                target = reward\n",
        "            else:\n",
        "                target = reward + gamma * np.squeeze(next_critic_value)\n",
        "\n",
        "            advantage = target - np.squeeze(critic_value)\n",
        "\n",
        "            # Atualizar os modelos\n",
        "            update_actor_critic(\n",
        "                tf.convert_to_tensor(state, dtype=tf.float32),\n",
        "                np.array([action]),\n",
        "                np.array([advantage]),\n",
        "                np.array([target])\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        # Salvar a recompensa do episódio\n",
        "        reward_history.append(episode_reward)\n",
        "        print(f'Episódio {episode + 1}/{max_episodes} - Recompensa: {episode_reward}')\n",
        "\n",
        "    return reward_history\n",
        "\n",
        "# Treinamento do A2C\n",
        "reward_history = train(env)\n",
        "\n",
        "# Plotando a curva de aprendizagem\n",
        "plt.plot(reward_history)\n",
        "plt.xlabel('Episódio')\n",
        "plt.ylabel('Recompensa')\n",
        "plt.title('Curva de Aprendizado')\n",
        "plt.show()\n",
        "\n",
        "# Função para gravar o vídeo de um episódio\n",
        "def record_video(env, model, video_path=\"cartpole_a2c.mp4\"):\n",
        "    images = []\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        img = env.render(mode='rgb_array')\n",
        "        images.append(img)\n",
        "        action_probs = model(np.reshape(state, [1, env.observation_space.shape[0]]))\n",
        "        action_probs = np.squeeze(action_probs)  # Garantir a forma correta\n",
        "        action = np.argmax(action_probs)\n",
        "        state, _, done, _ = env.step(action)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Criar e salvar o vídeo\n",
        "    clip = mpy.ImageSequenceClip(images, fps=30)\n",
        "    clip.write_videofile(video_path)\n",
        "\n",
        "# Gravar um episódio\n",
        "record_video(env, actor_model)"
      ]
    }
  ]
}